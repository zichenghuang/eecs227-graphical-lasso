{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Lasso Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Threshold Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_threshold (x, t):\n",
    "    if x > 0 and x > t:\n",
    "        return x - t\n",
    "    elif x < 0 and abs(x) > t:\n",
    "        return x + t\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence (old_W, new_W, S, threshold):\n",
    "    d = S.shape[0]\n",
    "    x = np.abs(old_W - new_W).mean()\n",
    "    # print x - threshold * (np.abs(S).sum() + np.abs(S.diagonal()).sum()) / (d * d - d)\n",
    "    if np.abs(old_W - new_W).mean() < threshold * (np.abs(S).sum() + np.abs(S.diagonal()).sum()) / (d * d - d):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphical_lasso (X, lambda_parameter = 0.01, max_iteration = 100, threshold = 0.01):\n",
    "    # X: the original data matrix\n",
    "    # lambda_parameter: hyperparameter for L1 penalty\n",
    "    # max_iteration: maximum iteration\n",
    "    # threshold: convergence threshold\n",
    "    \n",
    "    # check if hyperparameter is zero\n",
    "    if lambda_parameter == 0:  # when lambda = 0, simply reutrn the inverse of sample covariance matrix\n",
    "        return np.linalg.inv(np.cov(X.T))\n",
    "    \n",
    "    # if hyperparameter is not zero, go on to do the following\n",
    "    # step 1 in algorithm 9.1\n",
    "    p = X.shape[1]                          # number of features\n",
    "    S = np.cov(X.T)                         # sample covariance matrix\n",
    "    W = np.cov(X.T)                         # set the initial W matrix\n",
    "    precision = np.linalg.inv(np.cov(X.T))  # initialize the precision matrix\n",
    "    index = np.arange(p)                    # index used to partition the matrix W and S\n",
    "    \n",
    "    # step 2 in algorithm 9.1\n",
    "    for i in range(max_iteration):\n",
    "        #W_old = W  # later used to check for convergence\n",
    "        \n",
    "        for j in range(p):\n",
    "            \n",
    "            # partition W and S\n",
    "            W_11 = W[index != j].T[index != j]        # select W matrix's all but the jth row and column to form W_11\n",
    "            s_12 = S[j, index != j]                   # select the s12 from S, I actually select s12.T for easier dimension\n",
    "            B = np.zeros((p - 1, p))                  # used to store the beta coefficients\n",
    "            beta_j = - precision[index != j, j] / precision[j, j]  ##??## is this the right way to define the initial beta_j?\n",
    "            V = W[index != j].T[index != j]           # used in coordinate descent\n",
    "            \n",
    "            # pathwise coordinate descent\n",
    "            for n in range(max_iteration):\n",
    "                #beta_old = beta_j.copy()  # previous beta, used for checking convergence of beta_j\n",
    "                for k in range(p - 1): \n",
    "                    ##!!## this is adopted from 17.26 in Elements of Statistical Learning by Hastie, Tishirani, and Friedman\n",
    "                    beta_j[k] = soft_threshold(s_12[k] - np.dot(V[np.arange(p - 1) != j, k], beta_j[np.arange(p - 1) != j]), \n",
    "                                               lambda_parameter) / V[k, k]\n",
    "                    \n",
    "                #if np.abs(beta_j - beta_old).mean() < threshold: ####!!#### some better test for convergence of beta_j\n",
    "                #    B[np.arange(p - 1), j] = beta_j\n",
    "                #    break\n",
    "            \n",
    "            # store the beta coefficients for jth freature\n",
    "            B[np.arange(p - 1), j] = beta_j\n",
    "            \n",
    "            #else:\n",
    "            #    # this triggers if break command did not occur\n",
    "            #    print \"The coordinate descent did not converge. Try to increase the maximum number of iterations.\"\n",
    "                \n",
    "            # update the w_12\n",
    "            W[index != j, j] = np.dot(W_11, beta_j)\n",
    "            \n",
    "        #if check_convergence(W_old, W, S, threshold):\n",
    "        #    break\n",
    "            \n",
    "    #else:\n",
    "    #    # this triggers if break command did not occur\n",
    "    #    print \"The algorithm did not converge. Try to increase the maximum number of iterations.\"\n",
    "    \n",
    "    # update the precision matrix\n",
    "    # step 3 in algorithm 9.1\n",
    "    for j in range(p):\n",
    "        precision[j, j] = 1 / (W[j, j] - np.dot(W[index != j, j], B[np.arange(p - 1), j]))  # this is theta_hat_22\n",
    "        precision[index != j, j] = - B[np.arange(p - 1), j] * precision[j, j]               # this is theta_hat_12\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphical_lasso(A, 0.005) # try it with the A matrix specified in Sentiment.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
