{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/michael/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # For sentiment analysis\n",
    "import cPickle as pickle # For loaded dataset from pickle file\n",
    "import tqdm # Progress bar\n",
    "from collections import Counter # Handy addon\n",
    "from pprint import pprint # Useful to print JSON objects\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset of articles with introductions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This loads the file that you want, might take several seconds (up to a minute)\n",
    "\n",
    "with open(\"news_sentiment.pickle\", \"r\") as f:\n",
    "    articles = pickle.load(f)\n",
    "#print len(articles), \"articles were loaded\"\n",
    "#print \"Example article:\"\n",
    "#pprint(articles[1040])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate articles from the two stories\n",
    "ISIS_articles = []\n",
    "Brexit_articles = []\n",
    "for a in articles:\n",
    "    if a[\"news_topic\"] == 'ISIS War':\n",
    "        ISIS_articles.append(a)\n",
    "    else:\n",
    "        Brexit_articles.append(a)\n",
    "        \n",
    "#print len(ISIS_articles), \" articles from ISIS War and \", len(Brexit_articles), \"articles from Brexit were loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get only articles from one story, you can change this\n",
    "articles = ISIS_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract introductions, and obtain their sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "total_introductions = []\n",
    "for a in articles:\n",
    "    for intro in a.get('introductions', []):\n",
    "        intro['source'] = a['source']\n",
    "        total_introductions.append(intro)\n",
    "\n",
    "for intro in tqdm.tqdm_notebook(total_introductions):\n",
    "    intro['sentiment'] = analyzer.polarity_scores(intro['text'])['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example some sentiment for some of the introductions\n",
    "\n",
    "#subsample = np.random.choice(total_introductions, 100)\n",
    "#for intro in subsample:\n",
    "#    if intro['sentiment'] != 0:\n",
    "#        print \"---------------\"\n",
    "#        print \"Entity mentionned:\", intro['person']\n",
    "#        print intro['text']\n",
    "#        print \"Sentiment:\", intro['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a 2-dimensional object containing sentiment per entity, per source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ent_source_sent = {}\n",
    "\n",
    "for intro in total_introductions:\n",
    "    p = intro['person']\n",
    "    s = intro['source']\n",
    "    if p not in ent_source_sent:\n",
    "        ent_source_sent[p] = {}\n",
    "    if s not in ent_source_sent[p]:\n",
    "        ent_source_sent[p][s] = []\n",
    "    ent_source_sent[p][s].append(intro['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An example of how one entity (a city) is described by different sources\n",
    "\n",
    "#print ent_source_sent['Aleppo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will keep a total of 7852  /  25128 in our dataset\n",
      "We have  22 sources:  [u'telegraph.co.uk', u'foxnews.com', u'ap.org', u'businessinsider.in', u'independent.co.uk', u'reuters.com', u'wikinews.org', u'cnn.com', u'techcrunch.com', u'aa.com.tr', u'allafrica.com', u'nytimes.com', u'bloomberg.com', u'bbc.co.uk', u'latimes.com', u'rt.com', u'france24.com', u'chinadaily.com.cn', u'theguardian.com', u'washingtonpost.com', u'middleeasteye.net', u'aljazeera.com']\n"
     ]
    }
   ],
   "source": [
    "# We get rid of entities that don't contain enough data\n",
    "\n",
    "entities_kept = []\n",
    "\n",
    "for entity in ent_source_sent.keys():\n",
    "    sentiments = ent_source_sent[entity]\n",
    "    total_size = sum([len(sentiments[source]) for source in sentiments.keys()])\n",
    "    if total_size >= 3:\n",
    "        entities_kept.append(entity)\n",
    "        \n",
    "print \"We will keep a total of\", len(entities_kept), \" / \", len(ent_source_sent.keys()) ,\"in our dataset\"\n",
    "\n",
    "sources = set([])\n",
    "for entity in entities_kept:\n",
    "    sources|= set(ent_source_sent[entity].keys())\n",
    "sources = list(sources)\n",
    "\n",
    "print \"We have \", len(sources), \"sources: \", sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We create the array we will use in our sparse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We allocated some sentiment in this matrix, the repartition is: Counter({0: 19061, 1: 3650, -1: 2670})\n"
     ]
    }
   ],
   "source": [
    "# Parameters: changing these affects the results you get\n",
    "Pos_neg_ratio = 2.0\n",
    "overall_ratio = 0.15\n",
    "pos_threshold = 0.15\n",
    "neg_threshold = -0.15\n",
    "\n",
    "N = len(entities_kept)\n",
    "M = len(sources)\n",
    "A = np.zeros((N, M))\n",
    "\n",
    "sentiment_counts = Counter()\n",
    "\n",
    "source2j = {source: j for j, source in enumerate(sources)}\n",
    "\n",
    "for i, entity in enumerate(entities_kept):\n",
    "    for source in ent_source_sent[entity].keys():\n",
    "        sent_array = np.array(ent_source_sent[entity][source])\n",
    "        N_pos = float(len(np.where(sent_array > pos_threshold)[0]))\n",
    "        N_neg = float(len(np.where(sent_array < neg_threshold)[0]))\n",
    "        T = float(len(sent_array))\n",
    "        aggregate_sentiment = 0\n",
    "        if N_pos > Pos_neg_ratio*N_neg and N_pos > overall_ratio*T:\n",
    "            aggregate_sentiment = 1\n",
    "        elif N_neg > Pos_neg_ratio*N_pos and N_neg > overall_ratio*T:\n",
    "            aggregate_sentiment = -1\n",
    "        j = source2j[source]\n",
    "        \n",
    "        A[i,j] = aggregate_sentiment\n",
    "        \n",
    "        sentiment_counts[aggregate_sentiment] += 1\n",
    "\n",
    "print \"We allocated some sentiment in this matrix, the repartition is:\", sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7852L, 22L)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Lasso Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Threshold Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soft_threshold (x, t):\n",
    "    if x > 0:\n",
    "        return max(x-t, 0)\n",
    "    else:\n",
    "        return min(x+t, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_convergence (old_W, new_W, S, threshold):\n",
    "    d = S.shape[0]\n",
    "    x = np.abs(old_W - new_W).mean()\n",
    "    # print x - threshold * (np.abs(S).sum() + np.abs(S.diagonal()).sum()) / (d * d - d)\n",
    "    if np.abs(old_W - new_W).mean() < threshold * (np.abs(S).sum() + np.abs(S.diagonal()).sum()) / (d * d - d):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Lasso\n",
    "\n",
    "#### 05/03: ensured the resulting precision matrix to be symmetric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graphical_lasso (X, lambda_parameter = 0.01, max_iteration = 100, threshold = 1e-10):\n",
    "    ##################################################\n",
    "    # X: the original data matrix\n",
    "    # lambda_parameter: hyperparameter for L1 penalty\n",
    "    # max_iteration: maximum iteration\n",
    "    # threshold: convergence threshold\n",
    "    ##################################################\n",
    "    \n",
    "    S = np.cov(X.T)  # sample covariance matrix\n",
    "    \n",
    "    #### check whether hyperparameter is zero ####\n",
    "    \n",
    "    if lambda_parameter == 0:  # when lambda = 0, simply reutrn the inverse of sample covariance matrix\n",
    "        return np.linalg.inv(S)\n",
    "    \n",
    "    #### if hyperparameter is not zero, go on ####\n",
    "    \n",
    "    #### step 1 in algorithm 9.1 ####\n",
    "    p = X.shape[1]                          # number of features\n",
    "    W = np.cov(X.T)                         # set the initial W matrix\n",
    "    precision = np.linalg.inv(np.cov(X.T))  # initialize the precision matrix\n",
    "    index = np.arange(p)                    # index used to partition the matrix W and S\n",
    "    \n",
    "    #### step 2 in algorithm 9.1 ####\n",
    "    B = np.zeros((p - 1, p))  # used to store the beta coefficients\n",
    "    for i in range(max_iteration):\n",
    "        W_old = W.copy()  # later used to check for convergence\n",
    "        \n",
    "        for j in range(p):\n",
    "            index_j = index != j\n",
    "            # partition W and S\n",
    "            W_11 = W[index_j].T[index_j]        # select W matrix's all but the jth row and column to form W_11\n",
    "            s_12 = S[j, index_j]                   # select the s12 from S, I actually select s12.T for easier dimension\n",
    "            beta_j = - precision[index_j, j] / precision[j, j]  ##??## is this the right way to define the initial beta_j?\n",
    "            \n",
    "            # pathwise coordinate descent\n",
    "            for n in range(max_iteration):\n",
    "                beta_old = beta_j.copy()  # previous beta, used for checking convergence of beta_j\n",
    "                for k in range(p - 1): \n",
    "                    ##!!## this is adopted from 17.26 in Elements of Statistical Learning by Hastie, Tishirani, and Friedman\n",
    "                    beta_j[k] = soft_threshold(s_12[k] - np.dot(W_11[k], beta_j) + W_11[k, k] * beta_j[k],\n",
    "                                               lambda_parameter) / W_11[k, k]\n",
    "                \n",
    "                # convergence condition for coordinate descent ##??## how to break the whole function if not converged\n",
    "                if np.linalg.norm(beta_j - beta_old) < threshold:\n",
    "                    break\n",
    "            \n",
    "            # store the beta coefficients for jth freature\n",
    "            B[:, j] = beta_j\n",
    "            \n",
    "            # update the w_12\n",
    "            W[index_j, j] = np.dot(W_11, beta_j)\n",
    "            \n",
    "            if i == max_iteration - 1:\n",
    "                print \"The coordinate descent did not converge. Try to increase the maximum number of iterations.\"\n",
    "            \n",
    "        if np.linalg.norm(W - W_old) < threshold:\n",
    "            break\n",
    "            \n",
    "    #else:\n",
    "    #    # this triggers if break command did not occur\n",
    "    #    print \"The algorithm did not converge. Try to increase the maximum number of iterations.\"\n",
    "    \n",
    "    \n",
    "    #### step 3 in algorithm 9.1 ###\n",
    "    # update the precision matrix with result in the final round\n",
    "    for j in range(p):\n",
    "        precision[j, j] = 1 / (W[j, j] - np.dot(W[index != j, j], B[np.arange(p - 1), j]))  # this is theta_hat_22\n",
    "        precision[index != j, j] = - B[np.arange(p - 1), j] * precision[j, j]               # this is theta_hat_12\n",
    "    \n",
    "    return precision\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 40.2 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit result1 = graphical_lasso(A, 0.01, threshold = 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dif = result1 - result1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.9661793251323213e-12"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8832.1956763175967"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check with graphical_lasso from sklearn.covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.covariance import graph_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result2 = graph_lasso(np.cov(A.T), 0.01)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8832.195676317715"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.73949113136e-07\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print np.linalg.norm(result1 - result2)\n",
    "print np.sum(result1 != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model source similarity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Write code that uses this matrix (entities, sources) to compute\n",
    "# source similarity visible in bias of the way they describe entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {
    "778194441efa4fe0ac005b5453b5c790": {
     "views": [
      {
       "cell_index": 6
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
